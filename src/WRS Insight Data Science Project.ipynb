{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection on COVID-19 County-Level Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Setup***: of the > 3000 counties in the US, want to identify those most likely to become 'hotspots'. By this we don't mean places with 'a lot' of cases (such as NYC, which was *bound* to have high numbers, but also has high resources), but places showing anomalous growth in the infection rates, in some way weighted by population/resources. For example, this could identify a subset of places for investigators to focus their attention and for disaster relief managers to be vigilent about sending support.\n",
    "\n",
    "***Solution***: combine COVID-19 dataset from NYT's public repository (already cleaned, mostly), with Census Bureau's most recent (2018) estimate of population by county. Build features, run Local Outlier Factor analysis.\n",
    "\n",
    "## Building Dataset\n",
    "\n",
    "### Update NYT COVID-19 data git repository\n",
    "\n",
    "Data is from NYT publicly available US COVID-19 data at state and county-level, available [here](https://github.com/nytimes/covid-19-data). Note there are some [geographic exceptions](https://github.com/nytimes/covid-19-data#geographic-exceptions), which we will have to account for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# project home directory\n",
    "project_dir = os.path.abspath('') \n",
    "\n",
    "# run git pull on nyt's repository\n",
    "msg = subprocess.run(['git','pull'],\n",
    "                     cwd=os.path.join(project_dir,'nyt'),\n",
    "                     stdout=subprocess.PIPE)\n",
    "\n",
    "# print git output for good measure\n",
    "print(msg.stdout.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NYT COVID-19 data from file\n",
    "\n",
    "Data downloaded as CSV file, check data types on subsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date      object\n",
      "county    object\n",
      "state     object\n",
      "fips       int64\n",
      "cases      int64\n",
      "deaths     int64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>fips</th>\n",
       "      <th>cases</th>\n",
       "      <th>deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     county       state   fips  cases  deaths\n",
       "0  2020-01-21  Snohomish  Washington  53061      1       0\n",
       "1  2020-01-22  Snohomish  Washington  53061      1       0\n",
       "2  2020-01-23  Snohomish  Washington  53061      1       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = os.path.join(project_dir,'nyt','us-counties.csv')\n",
    "\n",
    "# load in nyt data, unprocessed\n",
    "df = pd.read_csv(filename, nrows=10)\n",
    "\n",
    "# look at data types\n",
    "print(df.dtypes)\n",
    "\n",
    "# take a look\n",
    "df.head(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix column datatypes, load all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>fips</th>\n",
       "      <th>cases</th>\n",
       "      <th>deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     county       state   fips  cases  deaths\n",
       "0 2020-01-21  Snohomish  Washington  53061      1       0\n",
       "1 2020-01-22  Snohomish  Washington  53061      1       0\n",
       "2 2020-01-23  Snohomish  Washington  53061      1       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# will need fips as string for later when combining with Census data\n",
    "col_dtypes = {'fips' : str}\n",
    "\n",
    "# also, parse dates as dates!\n",
    "df = pd.read_csv(filename, dtype=col_dtypes, parse_dates=[0])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Kansas City\n",
    "\n",
    "NYT data has split Kansas City from the four counties it lies in. To make fair comparison with census data, bin all four counties together along with Kansas City as `county='Kansas City'`, `fips='kscty'`.\n",
    "\n",
    "First, define helper function to massage the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# helper function to combine Kansas city with four surrounding counties\n",
    "def combine_kansas_city_covid(df,mo_dfs):\n",
    "\n",
    "    # initialize dataframe with Kansas City data\n",
    "    mo_df = df[df['county']=='Kansas City'] \n",
    "    \n",
    "    # set fips to custom 'kscty'\n",
    "    mo_df.loc[:,'fips'] = 'kscty'\n",
    "    \n",
    "    # loop over each of the counties and each of the halves of the merged dataframe\n",
    "    for d in mo_dfs:\n",
    "        \n",
    "        # outer join on 'date' and 'state'\n",
    "        mo_df = pd.merge(mo_df, d, on=['date','state'], how='outer')\n",
    "        \n",
    "        # set N/A cases and deaths from join to zero\n",
    "        for cases in ('cases_x','cases_y'): \n",
    "            mo_df[cases] = mo_df[cases].astype('Int64')\n",
    "            mo_df.loc[np.isnan(mo_df[cases]),[cases]]=0\n",
    "        for deaths in ('deaths_x','deaths_y'):\n",
    "            mo_df[deaths] = mo_df[deaths].astype('Int64')\n",
    "            mo_df.loc[np.isnan(mo_df[deaths]),[deaths]]=0\n",
    "\n",
    "        # total cases\n",
    "        mo_df['cases'] = (mo_df['cases_x'] + mo_df['cases_y']).map(int)\n",
    "        \n",
    "        # total deaths\n",
    "        mo_df['deaths'] = (mo_df['deaths_x'] + mo_df['deaths_y']).map(int)\n",
    "        \n",
    "        # set fips, county, state\n",
    "        mo_df['fips'] = 'kscty'\n",
    "        mo_df['county'] = 'Kansas City'\n",
    "        mo_df['state'] = 'Missouri'\n",
    "        \n",
    "        # keep only columns we need\n",
    "        mo_df = mo_df[['date','county','state','fips','deaths','cases']]\n",
    "        \n",
    "    # eliminate original 'Kansas City' data, since it's now in mo_df\n",
    "    df = df[df['county']!='Kansas City'] \n",
    "    \n",
    "    # add new kansas city+ rows\n",
    "    df = pd.concat([df,mo_df])\n",
    "    \n",
    "    # remove each of 'Cass', 'Clay', etc\n",
    "    for county in counties: \n",
    "       df = df[df['county']!=county]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply manipulation to the four effected counties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wrs/anaconda3/envs/data_science/lib/python3.7/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# these are the four counties in which Kansas City lies\n",
    "counties = ['Cass','Clay','Jackson','Platte']\n",
    "\n",
    "# closure of dataframes for each county surrounding Kansas City\n",
    "mo_dfs = [df[(df['county']==county) & (df['state']=='Missouri')] for county in counties]\n",
    "\n",
    "# new dataframe with Kansas City fixed\n",
    "covid_df = combine_kansas_city_covid(df,mo_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More NYT COVID-19 Cleanup\n",
    "\n",
    "Since in this dataset, NYC, which is *not* a county and has no `fips`, is its own entity, will assign artificial `fips` of `'nycty'`.\n",
    "\n",
    "Also, NYT dataset has one county entry of `Unknown`, which has a small number of cases that we ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>fips</th>\n",
       "      <th>cases</th>\n",
       "      <th>deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>Cook</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>17031</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>Kansas City</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>kscty</td>\n",
       "      <td>966</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>Kansas City</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>kscty</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>Kansas City</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>kscty</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>Kansas City</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>kscty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>Kansas City</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>kscty</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92775 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date       county       state   fips  cases  deaths\n",
       "0  2020-01-21    Snohomish  Washington  53061      1       0\n",
       "1  2020-01-22    Snohomish  Washington  53061      1       0\n",
       "2  2020-01-23    Snohomish  Washington  53061      1       0\n",
       "3  2020-01-24         Cook    Illinois  17031      1       0\n",
       "4  2020-01-24    Snohomish  Washington  53061      1       0\n",
       "..        ...          ...         ...    ...    ...     ...\n",
       "38 2020-04-27  Kansas City    Missouri  kscty    966      34\n",
       "39 2020-03-16  Kansas City    Missouri  kscty      1       0\n",
       "40 2020-03-17  Kansas City    Missouri  kscty      4       0\n",
       "41 2020-03-18  Kansas City    Missouri  kscty      5       0\n",
       "42 2020-03-19  Kansas City    Missouri  kscty      7       0\n",
       "\n",
       "[92775 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use custom fips for NYC b/c NYT data bins NYC too\n",
    "covid_df.loc[covid_df['county']=='New York City','fips'] = 'nycty'\n",
    "\n",
    "# remove unknown counties\n",
    "covid_df = covid_df[covid_df['county']!='Unknown']\n",
    "\n",
    "# take a look!\n",
    "covid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US county-level population projections (2018)\n",
    "\n",
    "Previously downloaded from US Census Bureau [dataset](https://www2.census.gov/programs-surveys/popest/datasets/2010-2018/counties/asrh/cc-est2018-alldata.csv), described [here](https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2010-2018/cc-est2018-alldata.pdf).\n",
    "\n",
    "First load sample for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /Users/wrs/Desktop/insight/project/src/cc-est2018-alldata.csv does not exist: '/Users/wrs/Desktop/insight/project/src/cc-est2018-alldata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d089c349e0c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cc-est2018-alldata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# check it out first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data_science/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data_science/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data_science/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data_science/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data_science/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /Users/wrs/Desktop/insight/project/src/cc-est2018-alldata.csv does not exist: '/Users/wrs/Desktop/insight/project/src/cc-est2018-alldata.csv'"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(project_dir,'cc-est2018-alldata.csv')\n",
    "\n",
    "df = pd.read_csv(filename, nrows=5) \n",
    "\n",
    "# check it out first\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix `fips` codes in Census data\n",
    "\n",
    "Note 5-digit `fips` codes in NYT set are STATE+COUNTY, while in Census data STATE and COUNTY are separate and not zero-padded. We correct this below so the two sets can be effectively combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns we care about\n",
    "cols = ['TOT_POP', 'COUNTY','AGEGRP','YEAR','STATE']\n",
    "\n",
    "df1 = pd.read_csv(filename, usecols=cols)\n",
    "\n",
    "# add new `fips` column by 2-digit state code and concatenating it with 3-digit county code\n",
    "df1['fips'] = df1['STATE'].map('{:02d}'.format) + df1['COUNTY'].map('{:03d}'.format)\n",
    "\n",
    "# take a look\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Most Recent Year (2018) from Census data, with age groups combined\n",
    "\n",
    "From reading Census docs, `YEAR=11` is 2018, `AGEGRP=0` is all ages.\n",
    "\n",
    "Also, we only need to get total population per county and county identifier, we discard the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[(df1['YEAR'] == 11) & (df1['AGEGRP']==0)][['fips','TOT_POP']]\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Census data, modify NYC and Kansas City to match NYT's COVID-19 data\n",
    "\n",
    "In NYT data, 5 counties that make up NYC are lumped into one entity, while four counties containing Kansas City have had the actual municipality subtracted.\n",
    "\n",
    "We don't have separate population data for Kansas City, so we lumped all counties into Kansas city in NYT COVID-19 data, and we will do the same here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict of nyc counties and fip codes\n",
    "nyc_counties = {\n",
    "    'Bronx'    : '36005',\n",
    "    'Kings'    : '36047',\n",
    "    'New York' : '36061',\n",
    "    'Queens'   : '36081',\n",
    "    'Richmond' : '36085',\n",
    "}\n",
    "\n",
    "# dict of kansas city counties and fip codes\n",
    "mo_counties = {\n",
    "    'Cass'    : '29037',\n",
    "    'Clay'    : '29047',\n",
    "    'Jackson' : '29095',\n",
    "    'Platte'  : '29165',\n",
    "}\n",
    "\n",
    "# dummy dataframe. New fips are `nycty` and `kscty` just like with COVID datafram\n",
    "df2 = pd.DataFrame({'fips': ['nycty','kscty'],'TOT_POP' : [0,0]})\n",
    "\n",
    "# add NYC counties together\n",
    "for fips in nyc_counties.values():\n",
    "     df2['TOT_POP'][0] += df1[df1['fips']==fips]['TOT_POP']\n",
    "\n",
    "# add Kansas City counties together\n",
    "for fips in mo_counties.values():\n",
    "     df2['TOT_POP'][1] += df1[df1['fips']==fips]['TOT_POP']\n",
    "\n",
    "# add new NYC and Kansas City rows\n",
    "census_df = pd.concat([df1, df2])\n",
    "\n",
    "# take a look!\n",
    "census_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Census and COVID datasets\n",
    "\n",
    "Add population of each county to COVID-19 data to create new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left join\n",
    "df = pd.merge(covid_df, census_df, on='fips', how='left')\n",
    "\n",
    "# rename 'TOT_POP' column to `population`\n",
    "df['population'] = df['TOT_POP'].astype('Int64')\n",
    "df = df.drop(columns=['TOT_POP'])\n",
    "\n",
    "# take a look\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "Here we build some features out of the data, run Outlier Detection, analyze data.\n",
    "\n",
    "### Setup Feature Parameters\n",
    "\n",
    "Specify various parameters that go into defining the features.\n",
    "Notably, `end_date` determines which day we are doing tha analysis for, defaults to today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# specify date to look at, defaults to today\n",
    "end_date = np.datetime64(datetime.datetime.today())\n",
    "# end_date = np.datetime64('2020-04-03')\n",
    "\n",
    "# number of days to average case and death numbers over\n",
    "n_days_window = 2\n",
    "\n",
    "# number of days after which an infected individual can be assumed to be not infectious\n",
    "# anectodally, about 3 weeks\n",
    "n_days_of_infection = 26\n",
    "\n",
    "# number of days between exposure and showing symptoms/becoming infectious\n",
    "n_days_for_incubation = 5\n",
    "\n",
    "# number of days from exposure to death (post-selected for those who die)\n",
    "# anecdotally about 10 days after showing symptoms\n",
    "n_days_to_death = 15\n",
    "\n",
    "# minimum number of cases a county must have over entire window to be included in the dataset\n",
    "min_num_cases = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build features\n",
    "\n",
    "Build a list of features out of `n_days_window`-length series of `new_cases`, `new_deaths`, `new_case_baseline` (how many cases were there `n_days_for_incubation` ago?), `new_death_baseline` (how many cases were there `n_days_to_death` ago?), and `population`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "\n",
    "# measure of R_0, i.e. rate of infection per capita infected (at time of exposure)\n",
    "def feature1(new_cases,new_deaths,\n",
    "             new_case_baseline,\n",
    "             new_death_baseline,\n",
    "             population,\n",
    "             cases,\n",
    "             deaths,):\n",
    "    return np.mean(new_cases/new_case_baseline)\n",
    "feature_list.append(feature1)               \n",
    "    \n",
    "# rate of death per capita infected (at time of exposure)\n",
    "def feature2(new_cases,new_deaths,\n",
    "             new_case_baseline,\n",
    "             new_death_baseline,\n",
    "             population,\n",
    "             cases,\n",
    "             deaths,):\n",
    "    return np.mean(new_deaths/new_death_baseline)\n",
    "feature_list.append(feature2)               \n",
    "\n",
    "# population of county (log, to make smoother and compress)\n",
    "def feature3(new_cases,new_deaths,\n",
    "             new_case_baseline,\n",
    "             new_death_baseline,\n",
    "             population,\n",
    "             cases,\n",
    "             deaths,):\n",
    "    return np.mean(np.log(population))\n",
    "feature_list.append(feature3)               \n",
    "\n",
    "# proxy for fraction of population that is infected (should be somewhat robust to poor testing)\n",
    "def feature4(new_cases,new_deaths,\n",
    "             new_case_baseline,\n",
    "             new_death_baseline,\n",
    "             population,\n",
    "             cases,\n",
    "             deaths,):\n",
    "    return np.mean(new_case_baseline/population)\n",
    "feature_list.append(feature4)               \n",
    "\n",
    "# proxy for fraction of population that has died (should be somewhat robust to poor testing)\n",
    "def feature5(new_cases,new_deaths,\n",
    "             new_case_baseline,\n",
    "             new_death_baseline,\n",
    "             population,\n",
    "             cases,\n",
    "             deaths,):\n",
    "    return np.mean(deaths/population)\n",
    "feature_list.append(feature5)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function finds last True expression in a boolean array\n",
    "def findlast(bool_array):\n",
    "    idx = len(bool_array)-1\n",
    "    bool = bool_array[idx]\n",
    "    while not bool:\n",
    "        idx -= 1\n",
    "        if idx <= 0:\n",
    "            break\n",
    "        bool = bool_array[idx]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique fips and associated county and state names\n",
    "FIPS, INDS = np.unique(np.array(df['fips']), return_index=True)\n",
    "COUNTIES = np.array(df['county'])[INDS]\n",
    "STATES = np.array(df['state'])[INDS]\n",
    "\n",
    "# initialize sample data\n",
    "X = []\n",
    "fips = []\n",
    "counties = []\n",
    "states = []\n",
    "\n",
    "for ind in range(len(FIPS)):\n",
    "    \n",
    "    fip = FIPS[ind]\n",
    "    \n",
    "    # select only one county\n",
    "    x = df[df['fips']==fip]\n",
    "    \n",
    "    # get dates for that county, and start and end indices for time-window\n",
    "    dates = np.array(x['date'])\n",
    "    start_date = end_date - np.timedelta64(n_days_window,'D')\n",
    "    start_idx = findlast(dates <= start_date)\n",
    "    end_idx = findlast(dates <= end_date)\n",
    "    \n",
    "    # throw away county if time series is not long enough to support features\n",
    "    if start_idx < max(n_days_of_infection, n_days_for_incubation, n_days_to_death):\n",
    "        continue\n",
    "\n",
    "    # helper functions for getting new_cases and deaths\n",
    "    def differential(array, window):\n",
    "        return array[start_idx:end_idx] - array[start_idx-window:end_idx-window]\n",
    "\n",
    "    # helper functions for get baslines\n",
    "    def shift(array, shift):\n",
    "        return array[start_idx-shift:end_idx-shift]\n",
    "\n",
    "    population = x['population'].mean()\n",
    "    cases = np.array(x['cases'])\n",
    "    deaths = np.array(x['deaths'])\n",
    "    \n",
    "    # if number of cases too small (set by min_num_cases), throw it away\n",
    "    if cases[start_idx] < min_num_cases:\n",
    "        continue\n",
    "        \n",
    "    # build list of counties that meet all criteria for keeping\n",
    "    fips.append(fip)\n",
    "    counties.append(COUNTIES[ind])\n",
    "    states.append(STATES[ind])\n",
    "    \n",
    "    # extract relevant dates\n",
    "    dates = np.array(x['date'])[start_idx:end_idx]\n",
    "    \n",
    "    # build feature inputs\n",
    "    currently_infected = differential(cases,n_days_of_infection)\n",
    "    new_cases = differential(cases,1)\n",
    "    new_deaths = differential(deaths,1)\n",
    "    new_case_baseline = shift(cases, n_days_for_incubation)\n",
    "    new_death_baseline = shift(cases, n_days_to_death)\n",
    "    cases = shift(cases, 0)\n",
    "    deaths = shift(deaths, 0)\n",
    "    \n",
    "    # compute features\n",
    "    features = []\n",
    "    for feature in feature_list:\n",
    "        features.append(feature(new_cases,new_deaths,\n",
    "                                new_case_baseline,\n",
    "                                new_death_baseline,\n",
    "                                population,\n",
    "                                cases,\n",
    "                                deaths,\n",
    "                               )\n",
    "                       )\n",
    "    \n",
    "    X.append(np.array(features))\n",
    "    \n",
    "# turn samples into ndarray, and list of counties and states, for boolean indexing later\n",
    "X = np.asarray(X)\n",
    "counties = np.asarray(counties)\n",
    "states = np.asarray(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data according to defined features\n",
    "\n",
    "This is to get a sense about whether these features could highlight anomalies or if they just show large cities, for example.\n",
    "\n",
    "First, normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# mean-center and normalize data, using 'robust' b/c outlier problem\n",
    "X = preprocessing.robust_scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(321)\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.xlabel(\"new case rate\")\n",
    "plt.ylabel(\"new death rate\")\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.scatter(X[:,1],X[:,2])\n",
    "plt.xlabel(\"new death rate\")\n",
    "plt.ylabel(\"population\")\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.scatter(X[:,2],X[:,3])\n",
    "plt.xlabel(\"population\")\n",
    "plt.ylabel(\"fraction infected\")\n",
    "\n",
    "plt.subplot(324)\n",
    "plt.scatter(X[:,3],X[:,4])\n",
    "plt.xlabel(\"fraction infected\")\n",
    "plt.ylabel(\"fraction dead\")\n",
    "\n",
    "plt.subplot(325)\n",
    "plt.scatter(X[:,4],X[:,0])\n",
    "plt.xlabel(\"fraction dead\")\n",
    "plt.ylabel(\"new case rate\")\n",
    "\n",
    "plt.subplot(326)\n",
    "plt.scatter(X[:,3],X[:,1])\n",
    "plt.xlabel(\"fraction infected\")\n",
    "plt.ylabel(\"new death rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection\n",
    "\n",
    "Perform Local Outlier Factor analysis, sort states and county according to outlier score, print results.\n",
    "\n",
    "Top hit is Marion County, Ohio, where there is a known massive outbreak in a prison.\n",
    "Next is Waldo county in Maine, does not seem to be reflected in current news.\n",
    "Many of the following dozen or so are the locations of Meat-packing plants that have outbreaks, currently in the [news](https://www.usatoday.com/in-depth/news/investigations/2020/04/22/meat-packing-plants-covid-may-force-choice-worker-health-food/2995232001/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# run in parallel, auto-detect anomaly fraction\n",
    "lof = LocalOutlierFactor(n_jobs=-1, contamination=\"auto\")\n",
    "\n",
    "# detect outliers\n",
    "y_pred = lof.fit_predict(X)\n",
    "\n",
    "# get outlier scores\n",
    "scores = lof.negative_outlier_factor_\n",
    "perm = np.argsort(scores)\n",
    "scores_sorted = scores[perm]\n",
    "counties_sorted = counties[perm]\n",
    "states_sorted = states[perm]\n",
    "y_pred_sorted = y_pred[perm]\n",
    "\n",
    "# print results\n",
    "print('%i anomalies detected, %2.2f%% counties that meet case threhold:\\n' % (sum(y_pred==-1),100*sum(y_pred==-1)/len(counties)))\n",
    "print('%5s %20s %20s\\n' % ('SCORE','STATE','COUNTY'))\n",
    "for p in range(len(y_pred_sorted)):\n",
    "    if y_pred_sorted[p]==-1:\n",
    "        print('%5.2f %20s %20s' % (-scores_sorted[p], states_sorted[p], counties_sorted[p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
